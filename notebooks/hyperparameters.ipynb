{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding to path /home/igor/PycharmProjects/CodeGen\n"
     ]
    }
   ],
   "source": [
    "import os.path\n",
    "import time\n",
    "from itertools import product\n",
    "\n",
    "import itables\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "from codegen_sources.model.src.data.dictionary import Dictionary\n",
    "from codegen_sources.model.src.model import build_model\n",
    "from codegen_sources.model.src.utils import AttrDict\n",
    "\n",
    "N_LAYERS_ENCODER = N_LAYERS_DECODER = [2, 4, 6]\n",
    "EMB_DIM = [256, 512, 1024]\n",
    "\n",
    "MODEL_PATH = r\"/home/igor/PycharmProjects/CodeGen/training_artifacts/models/DOBF_var_shuffled.pth\"\n",
    "TMP_PATH = r\"/home/igor/PycharmProjects/CodeGen/training_artifacts/models/tmp.pth\"\n",
    "\n",
    "REPEAT = 10"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "def gen_random_batch(batch_size):\n",
    "    x1 = torch.randint(64000, size=(100, batch_size))\n",
    "    len1 = torch.randint(x1.size(0), size=(batch_size,))\n",
    "    langs1 = torch.ones_like(x1)\n",
    "    x2 = torch.randint(64000, size=(10, batch_size))\n",
    "    len2 = torch.randint(x2.size(0), size=(batch_size,))\n",
    "    langs2 = torch.ones_like(x2)\n",
    "    y = torch.randint(64000, size=(batch_size,))\n",
    "    pred_mask = torch.zeros_like(x2, dtype=torch.bool)\n",
    "    pred_mask[1, :] = True\n",
    "    return x1, len1, langs1, x2, len2, langs2, y, pred_mask\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def time_forward(encoder, decoder, langs1, langs2, len1, len2, pred_mask, spans, x1, x2, y):\n",
    "    # encode source sentence\n",
    "    total_start = time.perf_counter()\n",
    "    start = time.perf_counter()\n",
    "    for _ in range(REPEAT):\n",
    "        enc1 = encoder(\n",
    "            \"fwd\", x=x1, lengths=len1, langs=langs1, causal=False, spans=spans\n",
    "        )\n",
    "    time_enc = (time.perf_counter() - start) / REPEAT\n",
    "    enc1 = enc1.transpose(0, 1)\n",
    "    # decode target sentence\n",
    "    start = time.perf_counter()\n",
    "    for _ in range(REPEAT):\n",
    "        dec2 = decoder(\n",
    "            \"fwd\",\n",
    "            x=x2,\n",
    "            lengths=len2,\n",
    "            langs=langs2,\n",
    "            causal=True,\n",
    "            src_enc=enc1,\n",
    "            src_len=len1,\n",
    "            spans=spans,\n",
    "        )\n",
    "    time_dec = (time.perf_counter() - start) / REPEAT\n",
    "    # loss\n",
    "    start = time.perf_counter()\n",
    "    for _ in range(REPEAT):\n",
    "        scores, loss = decoder(\n",
    "            \"predict\", tensor=dec2, pred_mask=pred_mask, y=y, get_scores=True\n",
    "        )\n",
    "    time_pred = (time.perf_counter() - start) / REPEAT\n",
    "    total = (time.perf_counter() - total_start) / REPEAT\n",
    "    return {\"time_enc\": time_enc, \"time_dec\": time_dec, \"time_pred\": time_pred, \"total_time\": total}\n",
    "\n",
    "def _reload_model(model_path, gpu=False):\n",
    "    # reload model\n",
    "    reloaded = torch.load(model_path, map_location=\"cpu\")\n",
    "    # change params of the reloaded model so that it will\n",
    "    # relaod its own weights and not the MLM or DOBF pretrained model\n",
    "    reloaded[\"params\"][\"reload_model\"] = \",\".join([model_path] * 2)\n",
    "    reloaded[\"params\"][\"lgs_mapping\"] = \"\"\n",
    "    reloaded[\"params\"][\"reload_encoder_for_decoder\"] = False\n",
    "    reloaded_params = AttrDict(reloaded[\"params\"])\n",
    "\n",
    "    # build dictionary / update parameters\n",
    "    dico = Dictionary(\n",
    "        reloaded[\"dico_id2word\"], reloaded[\"dico_word2id\"], reloaded[\"dico_counts\"]\n",
    "    )\n",
    "\n",
    "    # build model / reload weights (in the build_model method)\n",
    "    return reloaded_params, dico, build_model(reloaded_params, dico, gpu)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Lang java_dictionary matched to pretrained java_dictionary lang embedding.\n",
      "WARNING:root:Lang java_obfuscated matched to pretrained java_obfuscated lang embedding.\n",
      "WARNING:root:Lang java_dictionary matched to pretrained java_dictionary lang embedding.\n",
      "WARNING:root:Lang java_obfuscated matched to pretrained java_obfuscated lang embedding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder: <bound method Module.parameters of TransformerModel(\n",
      "  (position_embeddings): Embedding(2048, 1024)\n",
      "  (lang_embeddings): Embedding(2, 1024)\n",
      "  (embeddings): Embedding(64000, 1024, padding_idx=2)\n",
      "  (layer_norm_emb): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (attentions): ModuleList(\n",
      "    (0): MultiHeadAttention(\n",
      "      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    )\n",
      "    (1): MultiHeadAttention(\n",
      "      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    )\n",
      "    (2): MultiHeadAttention(\n",
      "      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    )\n",
      "    (3): MultiHeadAttention(\n",
      "      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    )\n",
      "    (4): MultiHeadAttention(\n",
      "      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    )\n",
      "    (5): MultiHeadAttention(\n",
      "      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (layer_norm1): ModuleList(\n",
      "    (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    (1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    (2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    (3): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    (4): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    (5): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (ffns): ModuleList(\n",
      "    (0): TransformerFFN(\n",
      "      (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "      (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "    )\n",
      "    (1): TransformerFFN(\n",
      "      (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "      (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "    )\n",
      "    (2): TransformerFFN(\n",
      "      (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "      (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "    )\n",
      "    (3): TransformerFFN(\n",
      "      (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "      (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "    )\n",
      "    (4): TransformerFFN(\n",
      "      (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "      (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "    )\n",
      "    (5): TransformerFFN(\n",
      "      (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "      (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (layer_norm2): ModuleList(\n",
      "    (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    (1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    (2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    (3): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    (4): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    (5): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (pred_layer): PredLayer(\n",
      "    (proj): Linear(in_features=1024, out_features=64000, bias=True)\n",
      "  )\n",
      ")>\n",
      "Decoder: <bound method Module.parameters of TransformerModel(\n",
      "  (position_embeddings): Embedding(2048, 1024)\n",
      "  (lang_embeddings): Embedding(2, 1024)\n",
      "  (embeddings): Embedding(64000, 1024, padding_idx=2)\n",
      "  (layer_norm_emb): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (attentions): ModuleList(\n",
      "    (0): MultiHeadAttention(\n",
      "      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    )\n",
      "    (1): MultiHeadAttention(\n",
      "      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    )\n",
      "    (2): MultiHeadAttention(\n",
      "      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    )\n",
      "    (3): MultiHeadAttention(\n",
      "      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    )\n",
      "    (4): MultiHeadAttention(\n",
      "      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    )\n",
      "    (5): MultiHeadAttention(\n",
      "      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (layer_norm1): ModuleList(\n",
      "    (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    (1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    (2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    (3): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    (4): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    (5): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (ffns): ModuleList(\n",
      "    (0): TransformerFFN(\n",
      "      (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "      (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "    )\n",
      "    (1): TransformerFFN(\n",
      "      (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "      (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "    )\n",
      "    (2): TransformerFFN(\n",
      "      (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "      (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "    )\n",
      "    (3): TransformerFFN(\n",
      "      (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "      (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "    )\n",
      "    (4): TransformerFFN(\n",
      "      (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "      (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "    )\n",
      "    (5): TransformerFFN(\n",
      "      (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "      (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (layer_norm2): ModuleList(\n",
      "    (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    (1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    (2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    (3): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    (4): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    (5): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (layer_norm15): ModuleList(\n",
      "    (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    (1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    (2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    (3): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    (4): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    (5): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (encoder_attn): ModuleList(\n",
      "    (0): MultiHeadAttention(\n",
      "      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    )\n",
      "    (1): MultiHeadAttention(\n",
      "      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    )\n",
      "    (2): MultiHeadAttention(\n",
      "      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    )\n",
      "    (3): MultiHeadAttention(\n",
      "      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    )\n",
      "    (4): MultiHeadAttention(\n",
      "      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    )\n",
      "    (5): MultiHeadAttention(\n",
      "      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (pred_layer): PredLayer(\n",
      "    (proj): Linear(in_features=1024, out_features=64000, bias=True)\n",
      "  )\n",
      ")>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27/27 [01:48<00:00,  4.03s/it]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "params, dico, (encoder, decoder) = _reload_model(MODEL_PATH)\n",
    "encoder = encoder[0]\n",
    "decoder = decoder[0]\n",
    "print(\"Encoder:\", encoder.parameters)\n",
    "print(\"Decoder:\", decoder.parameters)\n",
    "encoder.eval()\n",
    "decoder.eval()\n",
    "x1, len1, langs1, x2, len2, langs2, y, pred_mask = gen_random_batch(batch_size)\n",
    "d = {}\n",
    "d[\"n_enc\"] = params.n_layers_encoder\n",
    "d[\"n_dec\"] = params.n_layers_decoder\n",
    "d[\"emb_dim\"] = params.emb_dim_encoder\n",
    "d.update(time_forward(encoder, decoder, langs1, langs2, len1, len2, pred_mask, None, x1, x2, y))\n",
    "torch.save((encoder.state_dict(), decoder.state_dict()), TMP_PATH)\n",
    "d[\"size\"] = os.path.getsize(TMP_PATH) / 1024 / 1024\n",
    "d[\"enc_ps\"] = sum([p.numel() for p in encoder.parameters() if p.requires_grad])\n",
    "d[\"dec_ps\"] = sum([p.numel() for p in decoder.parameters() if p.requires_grad])\n",
    "d[\"sum_ps\"] = d[\"enc_ps\"] + d[\"dec_ps\"]\n",
    "d[\"emb_ps\"] = encoder.embeddings.weight.numel()\n",
    "params.reload_model = ''\n",
    "del encoder, decoder\n",
    "res = {'dobf': d}\n",
    "for i, (n_layers_encoder, n_layers_decoder, emb_dim) in tqdm(\n",
    "        list(enumerate(product(N_LAYERS_ENCODER, N_LAYERS_DECODER, EMB_DIM)))):\n",
    "    d = {}\n",
    "    d[\"n_enc\"] = params.n_layers_encoder = n_layers_encoder\n",
    "    d[\"n_dec\"] = params.n_layers_decoder = n_layers_decoder\n",
    "    d[\"emb_dim\"] = params.emb_dim = params.emb_dim_encoder = params.emb_dim_decoder = emb_dim\n",
    "    encoder, decoder = build_model(params, dico, gpu=False)\n",
    "    encoder, decoder = encoder[0], decoder[0]\n",
    "    d.update(time_forward(encoder, decoder, langs1, langs2, len1, len2, pred_mask, None, x1, x2, y))\n",
    "    torch.save((encoder.state_dict(), decoder.state_dict()), TMP_PATH)\n",
    "    d[\"size\"] = os.path.getsize(TMP_PATH) / 1024 / 1024\n",
    "    d[\"enc_ps\"] = sum([p.numel() for p in encoder.parameters() if p.requires_grad])\n",
    "    d[\"dec_ps\"] = sum([p.numel() for p in decoder.parameters() if p.requires_grad])\n",
    "    d[\"sum_ps\"] = d[\"enc_ps\"] + d[\"dec_ps\"]\n",
    "    d[\"emb_ps\"] = encoder.embeddings.weight.numel()\n",
    "    res[i] = d"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "      n_enc  n_dec  emb_dim  time_enc  time_dec  time_pred  total_time  \\\ndobf      6      6     1024  0.543810  0.139082   0.012677    0.695569   \n0         2      2      256  0.014599  0.005066   0.004129    0.023795   \n1         2      2      512  0.042180  0.012898   0.007396    0.062476   \n2         2      2     1024  0.156485  0.047851   0.013473    0.217810   \n3         2      4      256  0.015000  0.009659   0.004276    0.028936   \n4         2      4      512  0.042696  0.025528   0.007617    0.075843   \n5         2      4     1024  0.157633  0.094940   0.013558    0.266133   \n6         2      6      256  0.014363  0.013875   0.004173    0.032413   \n7         2      6      512  0.042421  0.038838   0.007431    0.088692   \n8         2      6     1024  0.155652  0.143203   0.013679    0.312535   \n9         4      2      256  0.029355  0.005093   0.004218    0.038667   \n10        4      2      512  0.084411  0.013515   0.007354    0.105281   \n11        4      2     1024  0.313758  0.048569   0.013822    0.376149   \n12        4      4      256  0.043958  0.009712   0.004262    0.057932   \n13        4      4      512  0.084139  0.027092   0.007380    0.118612   \n14        4      4     1024  0.313347  0.098265   0.013536    0.425149   \n15        4      6      256  0.031544  0.014551   0.004260    0.050356   \n16        4      6      512  0.084583  0.042617   0.007519    0.134720   \n17        4      6     1024  0.316551  0.147307   0.013423    0.477282   \n18        6      2      256  0.044947  0.005016   0.004153    0.054116   \n19        6      2      512  0.127811  0.013878   0.007660    0.149350   \n20        6      2     1024  0.478733  0.049040   0.014090    0.541864   \n21        6      4      256  0.043573  0.009655   0.004428    0.057657   \n22        6      4      512  0.129287  0.028222   0.007378    0.164888   \n23        6      4     1024  0.487879  0.101290   0.013781    0.602951   \n24        6      6      256  0.045121  0.014478   0.004509    0.064109   \n25        6      6      512  0.131778  0.042255   0.007637    0.181671   \n26        6      6     1024  0.495022  0.199493   0.014509    0.709026   \n\n             size     enc_ps     dec_ps     sum_ps    emb_ps  \ndobf  1189.350755  143278592  168481280  311759872  65536000  \n0      143.588017   18552832   19080192   37633024  16384000  \n1      314.658330   40187392   42290688   82478080  32768000  \n2      740.798955   92893696  101294592  194188288  65536000  \n3      151.641198   18552832   21187072   39739904  16384000  \n4      346.748620   40187392   50698752   90886144  32768000  \n5      868.963464   92893696  134887936  227781632  65536000  \n6      159.694383   18552832   23293952   41846784  16384000  \n7      378.838914   40187392   59106816   99294208  32768000  \n8      997.127976   92893696  168481280  261374976  65536000  \n9      149.623231   20132352   19080192   39212544  16384000  \n10     338.718934   46492160   42290688   88782848  32768000  \n11     836.910340  118086144  101294592  219380736  65536000  \n12     157.676415   20132352   21187072   41319424  16384000  \n13     370.809228   46492160   50698752   97190912  32768000  \n14     965.074853  118086144  134887936  252974080  65536000  \n15     165.729600   20132352   23293952   43426304  16384000  \n16     402.899522   46492160   59106816  105598976  32768000  \n17    1093.239366  118086144  168481280  286567424  65536000  \n18     155.658448   21711872   19080192   40792064  16384000  \n19     362.779542   52796928   42290688   95087616  32768000  \n20     933.021729  143278592  101294592  244573184  65536000  \n21     163.711633   21711872   21187072   42898944  16384000  \n22     394.869836   52796928   50698752  103495680  32768000  \n23    1061.186242  143278592  134887936  278166528  65536000  \n24     171.764817   21711872   23293952   45005824  16384000  \n25     426.960130   52796928   59106816  111903744  32768000  \n26    1189.350755  143278592  168481280  311759872  65536000  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>n_enc</th>\n      <th>n_dec</th>\n      <th>emb_dim</th>\n      <th>time_enc</th>\n      <th>time_dec</th>\n      <th>time_pred</th>\n      <th>total_time</th>\n      <th>size</th>\n      <th>enc_ps</th>\n      <th>dec_ps</th>\n      <th>sum_ps</th>\n      <th>emb_ps</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>dobf</th>\n      <td>6</td>\n      <td>6</td>\n      <td>1024</td>\n      <td>0.543810</td>\n      <td>0.139082</td>\n      <td>0.012677</td>\n      <td>0.695569</td>\n      <td>1189.350755</td>\n      <td>143278592</td>\n      <td>168481280</td>\n      <td>311759872</td>\n      <td>65536000</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>2</td>\n      <td>2</td>\n      <td>256</td>\n      <td>0.014599</td>\n      <td>0.005066</td>\n      <td>0.004129</td>\n      <td>0.023795</td>\n      <td>143.588017</td>\n      <td>18552832</td>\n      <td>19080192</td>\n      <td>37633024</td>\n      <td>16384000</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>2</td>\n      <td>512</td>\n      <td>0.042180</td>\n      <td>0.012898</td>\n      <td>0.007396</td>\n      <td>0.062476</td>\n      <td>314.658330</td>\n      <td>40187392</td>\n      <td>42290688</td>\n      <td>82478080</td>\n      <td>32768000</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>2</td>\n      <td>1024</td>\n      <td>0.156485</td>\n      <td>0.047851</td>\n      <td>0.013473</td>\n      <td>0.217810</td>\n      <td>740.798955</td>\n      <td>92893696</td>\n      <td>101294592</td>\n      <td>194188288</td>\n      <td>65536000</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2</td>\n      <td>4</td>\n      <td>256</td>\n      <td>0.015000</td>\n      <td>0.009659</td>\n      <td>0.004276</td>\n      <td>0.028936</td>\n      <td>151.641198</td>\n      <td>18552832</td>\n      <td>21187072</td>\n      <td>39739904</td>\n      <td>16384000</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2</td>\n      <td>4</td>\n      <td>512</td>\n      <td>0.042696</td>\n      <td>0.025528</td>\n      <td>0.007617</td>\n      <td>0.075843</td>\n      <td>346.748620</td>\n      <td>40187392</td>\n      <td>50698752</td>\n      <td>90886144</td>\n      <td>32768000</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>2</td>\n      <td>4</td>\n      <td>1024</td>\n      <td>0.157633</td>\n      <td>0.094940</td>\n      <td>0.013558</td>\n      <td>0.266133</td>\n      <td>868.963464</td>\n      <td>92893696</td>\n      <td>134887936</td>\n      <td>227781632</td>\n      <td>65536000</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>2</td>\n      <td>6</td>\n      <td>256</td>\n      <td>0.014363</td>\n      <td>0.013875</td>\n      <td>0.004173</td>\n      <td>0.032413</td>\n      <td>159.694383</td>\n      <td>18552832</td>\n      <td>23293952</td>\n      <td>41846784</td>\n      <td>16384000</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>2</td>\n      <td>6</td>\n      <td>512</td>\n      <td>0.042421</td>\n      <td>0.038838</td>\n      <td>0.007431</td>\n      <td>0.088692</td>\n      <td>378.838914</td>\n      <td>40187392</td>\n      <td>59106816</td>\n      <td>99294208</td>\n      <td>32768000</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>2</td>\n      <td>6</td>\n      <td>1024</td>\n      <td>0.155652</td>\n      <td>0.143203</td>\n      <td>0.013679</td>\n      <td>0.312535</td>\n      <td>997.127976</td>\n      <td>92893696</td>\n      <td>168481280</td>\n      <td>261374976</td>\n      <td>65536000</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>4</td>\n      <td>2</td>\n      <td>256</td>\n      <td>0.029355</td>\n      <td>0.005093</td>\n      <td>0.004218</td>\n      <td>0.038667</td>\n      <td>149.623231</td>\n      <td>20132352</td>\n      <td>19080192</td>\n      <td>39212544</td>\n      <td>16384000</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>4</td>\n      <td>2</td>\n      <td>512</td>\n      <td>0.084411</td>\n      <td>0.013515</td>\n      <td>0.007354</td>\n      <td>0.105281</td>\n      <td>338.718934</td>\n      <td>46492160</td>\n      <td>42290688</td>\n      <td>88782848</td>\n      <td>32768000</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>4</td>\n      <td>2</td>\n      <td>1024</td>\n      <td>0.313758</td>\n      <td>0.048569</td>\n      <td>0.013822</td>\n      <td>0.376149</td>\n      <td>836.910340</td>\n      <td>118086144</td>\n      <td>101294592</td>\n      <td>219380736</td>\n      <td>65536000</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>4</td>\n      <td>4</td>\n      <td>256</td>\n      <td>0.043958</td>\n      <td>0.009712</td>\n      <td>0.004262</td>\n      <td>0.057932</td>\n      <td>157.676415</td>\n      <td>20132352</td>\n      <td>21187072</td>\n      <td>41319424</td>\n      <td>16384000</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>4</td>\n      <td>4</td>\n      <td>512</td>\n      <td>0.084139</td>\n      <td>0.027092</td>\n      <td>0.007380</td>\n      <td>0.118612</td>\n      <td>370.809228</td>\n      <td>46492160</td>\n      <td>50698752</td>\n      <td>97190912</td>\n      <td>32768000</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>4</td>\n      <td>4</td>\n      <td>1024</td>\n      <td>0.313347</td>\n      <td>0.098265</td>\n      <td>0.013536</td>\n      <td>0.425149</td>\n      <td>965.074853</td>\n      <td>118086144</td>\n      <td>134887936</td>\n      <td>252974080</td>\n      <td>65536000</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>4</td>\n      <td>6</td>\n      <td>256</td>\n      <td>0.031544</td>\n      <td>0.014551</td>\n      <td>0.004260</td>\n      <td>0.050356</td>\n      <td>165.729600</td>\n      <td>20132352</td>\n      <td>23293952</td>\n      <td>43426304</td>\n      <td>16384000</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>4</td>\n      <td>6</td>\n      <td>512</td>\n      <td>0.084583</td>\n      <td>0.042617</td>\n      <td>0.007519</td>\n      <td>0.134720</td>\n      <td>402.899522</td>\n      <td>46492160</td>\n      <td>59106816</td>\n      <td>105598976</td>\n      <td>32768000</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>4</td>\n      <td>6</td>\n      <td>1024</td>\n      <td>0.316551</td>\n      <td>0.147307</td>\n      <td>0.013423</td>\n      <td>0.477282</td>\n      <td>1093.239366</td>\n      <td>118086144</td>\n      <td>168481280</td>\n      <td>286567424</td>\n      <td>65536000</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>6</td>\n      <td>2</td>\n      <td>256</td>\n      <td>0.044947</td>\n      <td>0.005016</td>\n      <td>0.004153</td>\n      <td>0.054116</td>\n      <td>155.658448</td>\n      <td>21711872</td>\n      <td>19080192</td>\n      <td>40792064</td>\n      <td>16384000</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>6</td>\n      <td>2</td>\n      <td>512</td>\n      <td>0.127811</td>\n      <td>0.013878</td>\n      <td>0.007660</td>\n      <td>0.149350</td>\n      <td>362.779542</td>\n      <td>52796928</td>\n      <td>42290688</td>\n      <td>95087616</td>\n      <td>32768000</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>6</td>\n      <td>2</td>\n      <td>1024</td>\n      <td>0.478733</td>\n      <td>0.049040</td>\n      <td>0.014090</td>\n      <td>0.541864</td>\n      <td>933.021729</td>\n      <td>143278592</td>\n      <td>101294592</td>\n      <td>244573184</td>\n      <td>65536000</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>6</td>\n      <td>4</td>\n      <td>256</td>\n      <td>0.043573</td>\n      <td>0.009655</td>\n      <td>0.004428</td>\n      <td>0.057657</td>\n      <td>163.711633</td>\n      <td>21711872</td>\n      <td>21187072</td>\n      <td>42898944</td>\n      <td>16384000</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>6</td>\n      <td>4</td>\n      <td>512</td>\n      <td>0.129287</td>\n      <td>0.028222</td>\n      <td>0.007378</td>\n      <td>0.164888</td>\n      <td>394.869836</td>\n      <td>52796928</td>\n      <td>50698752</td>\n      <td>103495680</td>\n      <td>32768000</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>6</td>\n      <td>4</td>\n      <td>1024</td>\n      <td>0.487879</td>\n      <td>0.101290</td>\n      <td>0.013781</td>\n      <td>0.602951</td>\n      <td>1061.186242</td>\n      <td>143278592</td>\n      <td>134887936</td>\n      <td>278166528</td>\n      <td>65536000</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>6</td>\n      <td>6</td>\n      <td>256</td>\n      <td>0.045121</td>\n      <td>0.014478</td>\n      <td>0.004509</td>\n      <td>0.064109</td>\n      <td>171.764817</td>\n      <td>21711872</td>\n      <td>23293952</td>\n      <td>45005824</td>\n      <td>16384000</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>6</td>\n      <td>6</td>\n      <td>512</td>\n      <td>0.131778</td>\n      <td>0.042255</td>\n      <td>0.007637</td>\n      <td>0.181671</td>\n      <td>426.960130</td>\n      <td>52796928</td>\n      <td>59106816</td>\n      <td>111903744</td>\n      <td>32768000</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>6</td>\n      <td>6</td>\n      <td>1024</td>\n      <td>0.495022</td>\n      <td>0.199493</td>\n      <td>0.014509</td>\n      <td>0.709026</td>\n      <td>1189.350755</td>\n      <td>143278592</td>\n      <td>168481280</td>\n      <td>311759872</td>\n      <td>65536000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "res_df = pd.DataFrame.from_dict(res, orient='index')\n",
    "display(res_df)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<table id=\"98b7420e-d832-40bb-b754-94eade540b35\" class=\"display\"style=\"max-width:100%\"><thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>n_enc</th>\n      <th>n_dec</th>\n      <th>emb_dim</th>\n      <th>time_enc</th>\n      <th>time_dec</th>\n      <th>time_pred</th>\n      <th>total_time</th>\n      <th>size</th>\n      <th>enc_ps</th>\n      <th>dec_ps</th>\n      <th>sum_ps</th>\n      <th>emb_ps</th>\n    </tr>\n  </thead><tbody><tr><td>Loading... (need <a href=https://mwouts.github.io/itables/troubleshooting.html>help</a>?)</td></tr></tbody></table>\n<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.datatables.net/1.11.3/css/jquery.dataTables.min.css\">\n<style> table td {\n    text-overflow: ellipsis;\n    overflow: hidden;\n} </style>\n<style> table th {\n    text-overflow: ellipsis;\n    overflow: hidden;\n} </style>\n<script type=\"module\">\n    // Define the table data\n    const data = [[\"dobf\", 6, 6, 1024, 0.54381, 0.139082, 0.012677, 0.695569, 1189.350755, 143278592, 168481280, 311759872, 65536000], [\"0\", 2, 2, 256, 0.014599, 0.005066, 0.004129, 0.023795, 143.588017, 18552832, 19080192, 37633024, 16384000], [\"1\", 2, 2, 512, 0.04218, 0.012898, 0.007396, 0.062476, 314.65833, 40187392, 42290688, 82478080, 32768000], [\"2\", 2, 2, 1024, 0.156485, 0.047851, 0.013473, 0.21781, 740.798955, 92893696, 101294592, 194188288, 65536000], [\"3\", 2, 4, 256, 0.015, 0.009659, 0.004276, 0.028936, 151.641198, 18552832, 21187072, 39739904, 16384000], [\"4\", 2, 4, 512, 0.042696, 0.025528, 0.007617, 0.075843, 346.74862, 40187392, 50698752, 90886144, 32768000], [\"5\", 2, 4, 1024, 0.157633, 0.09494, 0.013558, 0.266133, 868.963464, 92893696, 134887936, 227781632, 65536000], [\"6\", 2, 6, 256, 0.014363, 0.013875, 0.004173, 0.032413, 159.694383, 18552832, 23293952, 41846784, 16384000], [\"7\", 2, 6, 512, 0.042421, 0.038838, 0.007431, 0.088692, 378.838914, 40187392, 59106816, 99294208, 32768000], [\"8\", 2, 6, 1024, 0.155652, 0.143203, 0.013679, 0.312535, 997.127976, 92893696, 168481280, 261374976, 65536000], [\"9\", 4, 2, 256, 0.029355, 0.005093, 0.004218, 0.038667, 149.623231, 20132352, 19080192, 39212544, 16384000], [\"10\", 4, 2, 512, 0.084411, 0.013515, 0.007354, 0.105281, 338.718934, 46492160, 42290688, 88782848, 32768000], [\"11\", 4, 2, 1024, 0.313758, 0.048569, 0.013822, 0.376149, 836.91034, 118086144, 101294592, 219380736, 65536000], [\"12\", 4, 4, 256, 0.043958, 0.009712, 0.004262, 0.057932, 157.676415, 20132352, 21187072, 41319424, 16384000], [\"13\", 4, 4, 512, 0.084139, 0.027092, 0.00738, 0.118612, 370.809228, 46492160, 50698752, 97190912, 32768000], [\"14\", 4, 4, 1024, 0.313347, 0.098265, 0.013536, 0.425149, 965.074853, 118086144, 134887936, 252974080, 65536000], [\"15\", 4, 6, 256, 0.031544, 0.014551, 0.00426, 0.050356, 165.7296, 20132352, 23293952, 43426304, 16384000], [\"16\", 4, 6, 512, 0.084583, 0.042617, 0.007519, 0.13472, 402.899522, 46492160, 59106816, 105598976, 32768000], [\"17\", 4, 6, 1024, 0.316551, 0.147307, 0.013423, 0.477282, 1093.239366, 118086144, 168481280, 286567424, 65536000], [\"18\", 6, 2, 256, 0.044947, 0.005016, 0.004153, 0.054116, 155.658448, 21711872, 19080192, 40792064, 16384000], [\"19\", 6, 2, 512, 0.127811, 0.013878, 0.00766, 0.14935, 362.779542, 52796928, 42290688, 95087616, 32768000], [\"20\", 6, 2, 1024, 0.478733, 0.04904, 0.01409, 0.541864, 933.021729, 143278592, 101294592, 244573184, 65536000], [\"21\", 6, 4, 256, 0.043573, 0.009655, 0.004428, 0.057657, 163.711633, 21711872, 21187072, 42898944, 16384000], [\"22\", 6, 4, 512, 0.129287, 0.028222, 0.007378, 0.164888, 394.869836, 52796928, 50698752, 103495680, 32768000], [\"23\", 6, 4, 1024, 0.487879, 0.10129, 0.013781, 0.602951, 1061.186242, 143278592, 134887936, 278166528, 65536000], [\"24\", 6, 6, 256, 0.045121, 0.014478, 0.004509, 0.064109, 171.764817, 21711872, 23293952, 45005824, 16384000], [\"25\", 6, 6, 512, 0.131778, 0.042255, 0.007637, 0.181671, 426.96013, 52796928, 59106816, 111903744, 32768000], [\"26\", 6, 6, 1024, 0.495022, 0.199493, 0.014509, 0.709026, 1189.350755, 143278592, 168481280, 311759872, 65536000]];\n\n    if (typeof require === 'undefined') {\n        // TODO: This should become the default (use a simple import)\n        // when the ESM version works independently of whether\n        // require.js is there or not, see\n        // https://datatables.net/forums/discussion/69066/esm-es6-module-support?\n        const {default: $} = await import(\"https://esm.sh/jquery@3.5.0\");\n        const {default: initDataTables} = await import(\"https://esm.sh/datatables.net@1.11.3?deps=jquery@3.5.0\");\n\n        initDataTables();\n\n        // Define the dt_args\n        let dt_args = {};\n        dt_args[\"data\"] = data;\n\n        // Display the table\n        $(document).ready(function () {\n            $('#98b7420e-d832-40bb-b754-94eade540b35').DataTable(dt_args);\n        });\n    } else {\n        require([\"jquery\", \"datatables\"], ($, datatables) => {\n                // Define the dt_args\n                let dt_args = {};\n                dt_args[\"data\"] = data;\n\n                // Display the table\n                $(document).ready(function () {\n                    $('#98b7420e-d832-40bb-b754-94eade540b35').DataTable(dt_args);\n                });\n            }\n        )\n    }\n</script>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "itables.show(res_df)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}