{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding to path /home/igor/PycharmProjects/CodeGen\n"
     ]
    }
   ],
   "source": [
    "import os.path\n",
    "import time\n",
    "from itertools import product\n",
    "\n",
    "import itables\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "from codegen_sources.model.deobfuscate import _reload_model\n",
    "from codegen_sources.model.src.model import build_model\n",
    "from iren.inference.mute import mute_stdout_stderr\n",
    "\n",
    "N_LAYERS_ENCODER = N_LAYERS_DECODER = [2, 4, 6]\n",
    "EMB_DIM = [256, 512, 1024]\n",
    "\n",
    "MODEL_PATH = r\"/home/igor/PycharmProjects/CodeGen/training_artifacts/models/DOBF_var_shuffled.pth\"\n",
    "TMP_PATH = r\"/home/igor/PycharmProjects/CodeGen/training_artifacts/models/tmp.pth\"\n",
    "\n",
    "REPEAT = 10"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "def gen_random_batch(batch_size):\n",
    "    x1 = torch.randint(64000, size=(100, batch_size))\n",
    "    len1 = torch.randint(x1.size(0), size=(batch_size,))\n",
    "    langs1 = torch.ones_like(x1)\n",
    "    x2 = torch.randint(64000, size=(10, batch_size))\n",
    "    len2 = torch.randint(x2.size(0), size=(batch_size,))\n",
    "    langs2 = torch.ones_like(x2)\n",
    "    y = torch.randint(64000, size=(batch_size,))\n",
    "    pred_mask = torch.zeros_like(x2, dtype=torch.bool)\n",
    "    pred_mask[1, :] = True\n",
    "    return x1, len1, langs1, x2, len2, langs2, y, pred_mask\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def time_forward(encoder, decoder, langs1, langs2, len1, len2, pred_mask, spans, x1, x2, y):\n",
    "    # encode source sentence\n",
    "    total_start = time.perf_counter()\n",
    "    start = time.perf_counter()\n",
    "    for _ in range(REPEAT):\n",
    "        enc1 = encoder(\n",
    "            \"fwd\", x=x1, lengths=len1, langs=langs1, causal=False, spans=spans\n",
    "        )\n",
    "    time_enc = (time.perf_counter() - start) / REPEAT\n",
    "    enc1 = enc1.transpose(0, 1)\n",
    "    # decode target sentence\n",
    "    start = time.perf_counter()\n",
    "    for _ in range(REPEAT):\n",
    "        dec2 = decoder(\n",
    "            \"fwd\",\n",
    "            x=x2,\n",
    "            lengths=len2,\n",
    "            langs=langs2,\n",
    "            causal=True,\n",
    "            src_enc=enc1,\n",
    "            src_len=len1,\n",
    "            spans=spans,\n",
    "        )\n",
    "    time_dec = (time.perf_counter() - start) / REPEAT\n",
    "    # loss\n",
    "    start = time.perf_counter()\n",
    "    for _ in range(REPEAT):\n",
    "        scores, loss = decoder(\n",
    "            \"predict\", tensor=dec2, pred_mask=pred_mask, y=y, get_scores=True\n",
    "        )\n",
    "    time_pred = (time.perf_counter() - start) / REPEAT\n",
    "    total = (time.perf_counter() - total_start) / REPEAT\n",
    "    return {\"time_enc\": time_enc, \"time_dec\": time_dec, \"time_pred\": time_pred, \"total_time\": total}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 04/25/22 13:17:40 - 0:00:07 - ============ Model Reloading\n",
      "INFO - 04/25/22 13:17:40 - 0:00:07 - Reloading encoder from /home/igor/PycharmProjects/CodeGen/training_artifacts/models/DOBF_var_shuffled.pth ...\n",
      "WARNING - 04/25/22 13:17:41 - 0:00:09 - Lang java_dictionary matched to pretrained java_dictionary lang embedding.\n",
      "WARNING - 04/25/22 13:17:41 - 0:00:09 - Lang java_obfuscated matched to pretrained java_obfuscated lang embedding.\n",
      "INFO - 04/25/22 13:17:42 - 0:00:09 - Reloading decoders from /home/igor/PycharmProjects/CodeGen/training_artifacts/models/DOBF_var_shuffled.pth ...\n",
      "WARNING - 04/25/22 13:17:43 - 0:00:10 - Lang java_dictionary matched to pretrained java_dictionary lang embedding.\n",
      "WARNING - 04/25/22 13:17:43 - 0:00:10 - Lang java_obfuscated matched to pretrained java_obfuscated lang embedding.\n",
      "INFO - 04/25/22 13:17:43 - 0:00:10 - Number of parameters (encoder): 143278592\n",
      "INFO - 04/25/22 13:17:43 - 0:00:10 - Number of parameters (decoders): 168481280\n",
      "INFO - 04/25/22 13:17:43 - 0:00:10 - Number of decoders: 1\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder: <bound method Module.parameters of TransformerModel(\n",
      "  (position_embeddings): Embedding(2048, 1024)\n",
      "  (lang_embeddings): Embedding(2, 1024)\n",
      "  (embeddings): Embedding(64000, 1024, padding_idx=2)\n",
      "  (layer_norm_emb): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (attentions): ModuleList(\n",
      "    (0): MultiHeadAttention(\n",
      "      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    )\n",
      "    (1): MultiHeadAttention(\n",
      "      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    )\n",
      "    (2): MultiHeadAttention(\n",
      "      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    )\n",
      "    (3): MultiHeadAttention(\n",
      "      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    )\n",
      "    (4): MultiHeadAttention(\n",
      "      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    )\n",
      "    (5): MultiHeadAttention(\n",
      "      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (layer_norm1): ModuleList(\n",
      "    (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    (1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    (2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    (3): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    (4): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    (5): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (ffns): ModuleList(\n",
      "    (0): TransformerFFN(\n",
      "      (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "      (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "    )\n",
      "    (1): TransformerFFN(\n",
      "      (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "      (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "    )\n",
      "    (2): TransformerFFN(\n",
      "      (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "      (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "    )\n",
      "    (3): TransformerFFN(\n",
      "      (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "      (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "    )\n",
      "    (4): TransformerFFN(\n",
      "      (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "      (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "    )\n",
      "    (5): TransformerFFN(\n",
      "      (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "      (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (layer_norm2): ModuleList(\n",
      "    (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    (1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    (2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    (3): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    (4): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    (5): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (pred_layer): PredLayer(\n",
      "    (proj): Linear(in_features=1024, out_features=64000, bias=True)\n",
      "  )\n",
      ")>\n",
      "Decoder: <bound method Module.parameters of TransformerModel(\n",
      "  (position_embeddings): Embedding(2048, 1024)\n",
      "  (lang_embeddings): Embedding(2, 1024)\n",
      "  (embeddings): Embedding(64000, 1024, padding_idx=2)\n",
      "  (layer_norm_emb): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (attentions): ModuleList(\n",
      "    (0): MultiHeadAttention(\n",
      "      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    )\n",
      "    (1): MultiHeadAttention(\n",
      "      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    )\n",
      "    (2): MultiHeadAttention(\n",
      "      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    )\n",
      "    (3): MultiHeadAttention(\n",
      "      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    )\n",
      "    (4): MultiHeadAttention(\n",
      "      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    )\n",
      "    (5): MultiHeadAttention(\n",
      "      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (layer_norm1): ModuleList(\n",
      "    (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    (1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    (2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    (3): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    (4): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    (5): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (ffns): ModuleList(\n",
      "    (0): TransformerFFN(\n",
      "      (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "      (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "    )\n",
      "    (1): TransformerFFN(\n",
      "      (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "      (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "    )\n",
      "    (2): TransformerFFN(\n",
      "      (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "      (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "    )\n",
      "    (3): TransformerFFN(\n",
      "      (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "      (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "    )\n",
      "    (4): TransformerFFN(\n",
      "      (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "      (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "    )\n",
      "    (5): TransformerFFN(\n",
      "      (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "      (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (layer_norm2): ModuleList(\n",
      "    (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    (1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    (2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    (3): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    (4): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    (5): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (layer_norm15): ModuleList(\n",
      "    (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    (1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    (2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    (3): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    (4): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    (5): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (encoder_attn): ModuleList(\n",
      "    (0): MultiHeadAttention(\n",
      "      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    )\n",
      "    (1): MultiHeadAttention(\n",
      "      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    )\n",
      "    (2): MultiHeadAttention(\n",
      "      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    )\n",
      "    (3): MultiHeadAttention(\n",
      "      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    )\n",
      "    (4): MultiHeadAttention(\n",
      "      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    )\n",
      "    (5): MultiHeadAttention(\n",
      "      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (pred_layer): PredLayer(\n",
      "    (proj): Linear(in_features=1024, out_features=64000, bias=True)\n",
      "  )\n",
      ")>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/27 [00:00<?, ?it/s]INFO - 04/25/22 13:17:52 - 0:00:19 - Number of parameters (encoder): 18552832\n",
      "INFO - 04/25/22 13:17:52 - 0:00:19 - Number of parameters (decoders): 19080192\n",
      "INFO - 04/25/22 13:17:52 - 0:00:19 - Number of decoders: 1\n",
      "\n",
      "  4%|▎         | 1/27 [00:01<00:29,  1.15s/it]INFO - 04/25/22 13:17:53 - 0:00:21 - Number of parameters (encoder): 40187392\n",
      "INFO - 04/25/22 13:17:53 - 0:00:21 - Number of parameters (decoders): 42290688\n",
      "INFO - 04/25/22 13:17:53 - 0:00:21 - Number of decoders: 1\n",
      "\n",
      "  7%|▋         | 2/27 [00:03<00:41,  1.67s/it]INFO - 04/25/22 13:17:57 - 0:00:24 - Number of parameters (encoder): 92893696\n",
      "INFO - 04/25/22 13:17:57 - 0:00:24 - Number of parameters (decoders): 101294592\n",
      "INFO - 04/25/22 13:17:57 - 0:00:24 - Number of decoders: 1\n",
      "\n",
      " 11%|█         | 3/27 [00:08<01:21,  3.40s/it]INFO - 04/25/22 13:18:00 - 0:00:28 - Number of parameters (encoder): 18552832\n",
      "INFO - 04/25/22 13:18:00 - 0:00:28 - Number of parameters (decoders): 21187072\n",
      "INFO - 04/25/22 13:18:00 - 0:00:28 - Number of decoders: 1\n",
      "\n",
      " 15%|█▍        | 4/27 [00:09<00:57,  2.48s/it]INFO - 04/25/22 13:18:02 - 0:00:29 - Number of parameters (encoder): 40187392\n",
      "INFO - 04/25/22 13:18:02 - 0:00:29 - Number of parameters (decoders): 50698752\n",
      "INFO - 04/25/22 13:18:02 - 0:00:29 - Number of decoders: 1\n",
      "\n",
      " 19%|█▊        | 5/27 [00:11<00:52,  2.38s/it]INFO - 04/25/22 13:18:06 - 0:00:33 - Number of parameters (encoder): 92893696\n",
      "INFO - 04/25/22 13:18:06 - 0:00:33 - Number of parameters (decoders): 134887936\n",
      "INFO - 04/25/22 13:18:06 - 0:00:33 - Number of decoders: 1\n",
      "\n",
      " 22%|██▏       | 6/27 [00:17<01:15,  3.61s/it]INFO - 04/25/22 13:18:10 - 0:00:37 - Number of parameters (encoder): 18552832\n",
      "INFO - 04/25/22 13:18:10 - 0:00:37 - Number of parameters (decoders): 23293952\n",
      "INFO - 04/25/22 13:18:10 - 0:00:37 - Number of decoders: 1\n",
      "\n",
      " 26%|██▌       | 7/27 [00:19<00:55,  2.80s/it]INFO - 04/25/22 13:18:11 - 0:00:39 - Number of parameters (encoder): 40187392\n",
      "INFO - 04/25/22 13:18:11 - 0:00:39 - Number of parameters (decoders): 59106816\n",
      "INFO - 04/25/22 13:18:11 - 0:00:39 - Number of decoders: 1\n",
      "\n",
      " 30%|██▉       | 8/27 [00:21<00:51,  2.69s/it]INFO - 04/25/22 13:18:15 - 0:00:43 - Number of parameters (encoder): 92893696\n",
      "INFO - 04/25/22 13:18:15 - 0:00:43 - Number of parameters (decoders): 168481280\n",
      "INFO - 04/25/22 13:18:15 - 0:00:43 - Number of decoders: 1\n",
      "\n",
      " 33%|███▎      | 9/27 [00:28<01:13,  4.10s/it]INFO - 04/25/22 13:18:21 - 0:00:48 - Number of parameters (encoder): 20132352\n",
      "INFO - 04/25/22 13:18:21 - 0:00:48 - Number of parameters (decoders): 19080192\n",
      "INFO - 04/25/22 13:18:21 - 0:00:48 - Number of decoders: 1\n",
      "\n",
      " 37%|███▋      | 10/27 [00:29<00:54,  3.21s/it]INFO - 04/25/22 13:18:22 - 0:00:49 - Number of parameters (encoder): 46492160\n",
      "INFO - 04/25/22 13:18:22 - 0:00:49 - Number of parameters (decoders): 42290688\n",
      "INFO - 04/25/22 13:18:22 - 0:00:49 - Number of decoders: 1\n",
      "\n",
      " 41%|████      | 11/27 [00:32<00:47,  3.00s/it]INFO - 04/25/22 13:18:26 - 0:00:53 - Number of parameters (encoder): 118086144\n",
      "INFO - 04/25/22 13:18:26 - 0:00:53 - Number of parameters (decoders): 101294592\n",
      "INFO - 04/25/22 13:18:26 - 0:00:53 - Number of decoders: 1\n",
      "\n",
      " 44%|████▍     | 12/27 [00:39<01:05,  4.36s/it]INFO - 04/25/22 13:18:32 - 0:00:59 - Number of parameters (encoder): 20132352\n",
      "INFO - 04/25/22 13:18:32 - 0:00:59 - Number of parameters (decoders): 21187072\n",
      "INFO - 04/25/22 13:18:32 - 0:00:59 - Number of decoders: 1\n",
      "\n",
      " 48%|████▊     | 13/27 [00:41<00:48,  3.46s/it]INFO - 04/25/22 13:18:34 - 0:01:01 - Number of parameters (encoder): 46492160\n",
      "INFO - 04/25/22 13:18:34 - 0:01:01 - Number of parameters (decoders): 50698752\n",
      "INFO - 04/25/22 13:18:34 - 0:01:01 - Number of decoders: 1\n",
      "\n",
      " 52%|█████▏    | 14/27 [00:44<00:43,  3.38s/it]INFO - 04/25/22 13:18:38 - 0:01:06 - Number of parameters (encoder): 118086144\n",
      "INFO - 04/25/22 13:18:38 - 0:01:06 - Number of parameters (decoders): 134887936\n",
      "INFO - 04/25/22 13:18:38 - 0:01:06 - Number of decoders: 1\n",
      "\n",
      " 56%|█████▌    | 15/27 [00:52<00:58,  4.90s/it]INFO - 04/25/22 13:18:45 - 0:01:12 - Number of parameters (encoder): 20132352\n",
      "INFO - 04/25/22 13:18:45 - 0:01:12 - Number of parameters (decoders): 23293952\n",
      "INFO - 04/25/22 13:18:45 - 0:01:12 - Number of decoders: 1\n",
      "\n",
      " 59%|█████▉    | 16/27 [00:54<00:42,  3.82s/it]INFO - 04/25/22 13:18:47 - 0:01:14 - Number of parameters (encoder): 46492160\n",
      "INFO - 04/25/22 13:18:47 - 0:01:14 - Number of parameters (decoders): 59106816\n",
      "INFO - 04/25/22 13:18:47 - 0:01:14 - Number of decoders: 1\n",
      "\n",
      " 63%|██████▎   | 17/27 [00:57<00:37,  3.73s/it]INFO - 04/25/22 13:18:52 - 0:01:19 - Number of parameters (encoder): 118086144\n",
      "INFO - 04/25/22 13:18:52 - 0:01:19 - Number of parameters (decoders): 168481280\n",
      "INFO - 04/25/22 13:18:52 - 0:01:19 - Number of decoders: 1\n",
      "\n",
      " 67%|██████▋   | 18/27 [01:10<00:56,  6.31s/it]INFO - 04/25/22 13:19:02 - 0:01:29 - Number of parameters (encoder): 21711872\n",
      "INFO - 04/25/22 13:19:02 - 0:01:29 - Number of parameters (decoders): 19080192\n",
      "INFO - 04/25/22 13:19:02 - 0:01:29 - Number of decoders: 1\n",
      "\n",
      " 70%|███████   | 19/27 [01:11<00:38,  4.82s/it]INFO - 04/25/22 13:19:04 - 0:01:31 - Number of parameters (encoder): 52796928\n",
      "INFO - 04/25/22 13:19:04 - 0:01:31 - Number of parameters (decoders): 42290688\n",
      "INFO - 04/25/22 13:19:04 - 0:01:31 - Number of decoders: 1\n",
      "\n",
      " 74%|███████▍  | 20/27 [01:14<00:31,  4.43s/it]INFO - 04/25/22 13:19:09 - 0:01:36 - Number of parameters (encoder): 143278592\n",
      "INFO - 04/25/22 13:19:09 - 0:01:36 - Number of parameters (decoders): 101294592\n",
      "INFO - 04/25/22 13:19:09 - 0:01:36 - Number of decoders: 1\n",
      "\n",
      " 78%|███████▊  | 21/27 [01:24<00:36,  6.04s/it]INFO - 04/25/22 13:19:17 - 0:01:44 - Number of parameters (encoder): 21711872\n",
      "INFO - 04/25/22 13:19:17 - 0:01:44 - Number of parameters (decoders): 21187072\n",
      "INFO - 04/25/22 13:19:17 - 0:01:44 - Number of decoders: 1\n",
      "\n",
      " 81%|████████▏ | 22/27 [01:26<00:23,  4.71s/it]INFO - 04/25/22 13:19:19 - 0:01:46 - Number of parameters (encoder): 52796928\n",
      "INFO - 04/25/22 13:19:19 - 0:01:46 - Number of parameters (decoders): 50698752\n",
      "INFO - 04/25/22 13:19:19 - 0:01:46 - Number of decoders: 1\n",
      "\n",
      " 85%|████████▌ | 23/27 [01:29<00:17,  4.27s/it]INFO - 04/25/22 13:19:24 - 0:01:51 - Number of parameters (encoder): 143278592\n",
      "INFO - 04/25/22 13:19:24 - 0:01:51 - Number of parameters (decoders): 134887936\n",
      "INFO - 04/25/22 13:19:24 - 0:01:51 - Number of decoders: 1\n",
      "\n",
      " 89%|████████▉ | 24/27 [01:40<00:18,  6.22s/it]INFO - 04/25/22 13:19:32 - 0:01:59 - Number of parameters (encoder): 21711872\n",
      "INFO - 04/25/22 13:19:32 - 0:01:59 - Number of parameters (decoders): 23293952\n",
      "INFO - 04/25/22 13:19:32 - 0:01:59 - Number of decoders: 1\n",
      "\n",
      " 93%|█████████▎| 25/27 [01:41<00:09,  4.81s/it]INFO - 04/25/22 13:19:34 - 0:02:02 - Number of parameters (encoder): 52796928\n",
      "INFO - 04/25/22 13:19:34 - 0:02:02 - Number of parameters (decoders): 59106816\n",
      "INFO - 04/25/22 13:19:34 - 0:02:02 - Number of decoders: 1\n",
      "\n",
      " 96%|█████████▋| 26/27 [01:45<00:04,  4.44s/it]INFO - 04/25/22 13:19:40 - 0:02:07 - Number of parameters (encoder): 143278592\n",
      "INFO - 04/25/22 13:19:40 - 0:02:07 - Number of parameters (decoders): 168481280\n",
      "INFO - 04/25/22 13:19:40 - 0:02:07 - Number of decoders: 1\n",
      "\n",
      "100%|██████████| 27/27 [02:02<00:00,  4.53s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<table id=\"fd85a666-51fb-4118-9f8e-a506adb47e49\" class=\"display\"style=\"max-width:100%\"><thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>n_enc</th>\n      <th>n_dec</th>\n      <th>emb_dim</th>\n      <th>time_enc</th>\n      <th>time_dec</th>\n      <th>time_pred</th>\n      <th>total_time</th>\n      <th>size</th>\n      <th>enc_ps</th>\n      <th>dec_ps</th>\n      <th>sum_ps</th>\n      <th>emb_ps</th>\n    </tr>\n  </thead><tbody><tr><td>Loading... (need <a href=https://mwouts.github.io/itables/troubleshooting.html>help</a>?)</td></tr></tbody></table>\n<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.datatables.net/1.11.3/css/jquery.dataTables.min.css\">\n<style> table td {\n    text-overflow: ellipsis;\n    overflow: hidden;\n} </style>\n<style> table th {\n    text-overflow: ellipsis;\n    overflow: hidden;\n} </style>\n<script type=\"module\">\n    // Define the table data\n    const data = [[\"dobf\", 6.0, 6.0, 1024.0, 0.507107, 0.150976, 0.013108, 0.671192, 1189.350755, 143278592.0, 168481280.0, 311759872.0, 65536000.0], [\"0\", 2.0, 2.0, 256.0, 0.025125, 0.007737, 0.007144, 0.040007, 143.588017, 18552832.0, 19080192.0, 37633024.0, 16384000.0], [\"1\", 2.0, 2.0, 512.0, 0.044655, 0.013081, 0.007354, 0.065091, 314.65833, 40187392.0, 42290688.0, 82478080.0, 32768000.0], [\"2\", 2.0, 2.0, 1024.0, 0.163752, 0.046548, 0.013202, 0.223503, 740.798955, 92893696.0, 101294592.0, 194188288.0, 65536000.0], [\"3\", 2.0, 4.0, 256.0, 0.016536, 0.009788, 0.004369, 0.030694, 151.641198, 18552832.0, 21187072.0, 39739904.0, 16384000.0], [\"4\", 2.0, 4.0, 512.0, 0.044072, 0.025316, 0.0069, 0.076289, 346.74862, 40187392.0, 50698752.0, 90886144.0, 32768000.0], [\"5\", 2.0, 4.0, 1024.0, 0.151751, 0.092725, 0.012756, 0.257233, 868.963464, 92893696.0, 134887936.0, 227781632.0, 65536000.0], [\"6\", 2.0, 6.0, 256.0, 0.014983, 0.013825, 0.00439, 0.033198, 159.694383, 18552832.0, 23293952.0, 41846784.0, 16384000.0], [\"7\", 2.0, 6.0, 512.0, 0.041761, 0.039534, 0.008281, 0.089577, 378.838914, 40187392.0, 59106816.0, 99294208.0, 32768000.0], [\"8\", 2.0, 6.0, 1024.0, 0.155257, 0.150857, 0.013446, 0.319561, 997.127976, 92893696.0, 168481280.0, 261374976.0, 65536000.0], [\"9\", 4.0, 2.0, 256.0, 0.034316, 0.005126, 0.004069, 0.043512, 149.623231, 20132352.0, 19080192.0, 39212544.0, 16384000.0], [\"10\", 4.0, 2.0, 512.0, 0.090946, 0.013178, 0.006973, 0.111097, 338.718934, 46492160.0, 42290688.0, 88782848.0, 32768000.0], [\"11\", 4.0, 2.0, 1024.0, 0.314239, 0.056716, 0.015749, 0.386704, 836.91034, 118086144.0, 101294592.0, 219380736.0, 65536000.0], [\"12\", 4.0, 4.0, 256.0, 0.035923, 0.014319, 0.0051, 0.055342, 157.676415, 20132352.0, 21187072.0, 41319424.0, 16384000.0], [\"13\", 4.0, 4.0, 512.0, 0.120847, 0.035778, 0.008954, 0.165579, 370.809228, 46492160.0, 50698752.0, 97190912.0, 32768000.0], [\"14\", 4.0, 4.0, 1024.0, 0.337209, 0.099107, 0.013415, 0.449731, 965.074853, 118086144.0, 134887936.0, 252974080.0, 65536000.0], [\"15\", 4.0, 6.0, 256.0, 0.029001, 0.014333, 0.004094, 0.047428, 165.7296, 20132352.0, 23293952.0, 43426304.0, 16384000.0], [\"16\", 4.0, 6.0, 512.0, 0.098578, 0.055811, 0.01078, 0.16517, 402.899522, 46492160.0, 59106816.0, 105598976.0, 32768000.0], [\"17\", 4.0, 6.0, 1024.0, 0.562907, 0.199294, 0.015866, 0.778067, 1093.239366, 118086144.0, 168481280.0, 286567424.0, 65536000.0], [\"18\", 6.0, 2.0, 256.0, 0.045789, 0.004892, 0.003955, 0.054637, 155.658448, 21711872.0, 19080192.0, 40792064.0, 16384000.0], [\"19\", 6.0, 2.0, 512.0, 0.133489, 0.016307, 0.007375, 0.157172, 362.779542, 52796928.0, 42290688.0, 95087616.0, 32768000.0], [\"20\", 6.0, 2.0, 1024.0, 0.520439, 0.05432, 0.012717, 0.587477, 933.021729, 143278592.0, 101294592.0, 244573184.0, 65536000.0], [\"21\", 6.0, 4.0, 256.0, 0.047526, 0.015898, 0.006213, 0.069639, 163.711633, 21711872.0, 21187072.0, 42898944.0, 16384000.0], [\"22\", 6.0, 4.0, 512.0, 0.126943, 0.026677, 0.007062, 0.160683, 394.869836, 52796928.0, 50698752.0, 103495680.0, 32768000.0], [\"23\", 6.0, 4.0, 1024.0, 0.523022, 0.111059, 0.013762, 0.647845, 1061.186242, 143278592.0, 134887936.0, 278166528.0, 65536000.0], [\"24\", 6.0, 6.0, 256.0, 0.043866, 0.014492, 0.004278, 0.062637, 171.764817, 21711872.0, 23293952.0, 45005824.0, 16384000.0], [\"25\", 6.0, 6.0, 512.0, 0.138926, 0.047207, 0.007194, 0.193327, 426.96013, 52796928.0, 59106816.0, 111903744.0, 32768000.0], [\"26\", 6.0, 6.0, 1024.0, 0.905927, 0.297409, 0.018003, 1.221342, 1189.350755, 143278592.0, 168481280.0, 311759872.0, 65536000.0]];\n\n    if (typeof require === 'undefined') {\n        // TODO: This should become the default (use a simple import)\n        // when the ESM version works independently of whether\n        // require.js is there or not, see\n        // https://datatables.net/forums/discussion/69066/esm-es6-module-support?\n        const {default: $} = await import(\"https://esm.sh/jquery@3.5.0\");\n        const {default: initDataTables} = await import(\"https://esm.sh/datatables.net@1.11.3?deps=jquery@3.5.0\");\n\n        initDataTables();\n\n        // Define the dt_args\n        let dt_args = {};\n        dt_args[\"data\"] = data;\n\n        // Display the table\n        $(document).ready(function () {\n            $('#fd85a666-51fb-4118-9f8e-a506adb47e49').DataTable(dt_args);\n        });\n    } else {\n        require([\"jquery\", \"datatables\"], ($, datatables) => {\n                // Define the dt_args\n                let dt_args = {};\n                dt_args[\"data\"] = data;\n\n                // Display the table\n                $(document).ready(function () {\n                    $('#fd85a666-51fb-4118-9f8e-a506adb47e49').DataTable(dt_args);\n                });\n            }\n        )\n    }\n</script>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "batch_size = 16\n",
    "params, dico, (encoder, decoder) = _reload_model(MODEL_PATH)\n",
    "encoder = encoder[0]\n",
    "decoder = decoder[0]\n",
    "print(\"Encoder:\", encoder.parameters)\n",
    "print(\"Decoder:\", decoder.parameters)\n",
    "encoder.eval()\n",
    "decoder.eval()\n",
    "x1, len1, langs1, x2, len2, langs2, y, pred_mask = gen_random_batch(batch_size)\n",
    "d = {}\n",
    "d[\"n_enc\"] = params.n_layers_encoder\n",
    "d[\"n_dec\"] = params.n_layers_decoder\n",
    "d[\"emb_dim\"] = params.emb_dim_encoder\n",
    "d.update(time_forward(encoder, decoder, langs1, langs2, len1, len2, pred_mask, None, x1, x2, y))\n",
    "torch.save((encoder.state_dict(), decoder.state_dict()), TMP_PATH)\n",
    "d[\"size\"] = os.path.getsize(TMP_PATH) / 1024 / 1024\n",
    "d[\"enc_ps\"] = sum([p.numel() for p in encoder.parameters() if p.requires_grad])\n",
    "d[\"dec_ps\"] = sum([p.numel() for p in decoder.parameters() if p.requires_grad])\n",
    "d[\"sum_ps\"] = d[\"enc_ps\"] + d[\"dec_ps\"]\n",
    "d[\"emb_ps\"] = encoder.embeddings.weight.numel()\n",
    "params.reload_model = ''\n",
    "del encoder, decoder\n",
    "res = {'dobf': d}\n",
    "for i, (n_layers_encoder, n_layers_decoder, emb_dim) in tqdm(\n",
    "        list(enumerate(product(N_LAYERS_ENCODER, N_LAYERS_DECODER, EMB_DIM)))):\n",
    "    d = {}\n",
    "    d[\"n_enc\"] = params.n_layers_encoder = n_layers_encoder\n",
    "    d[\"n_dec\"] = params.n_layers_decoder = n_layers_decoder\n",
    "    d[\"emb_dim\"] = params.emb_dim = params.emb_dim_encoder = params.emb_dim_decoder = emb_dim\n",
    "    with mute_stdout_stderr():\n",
    "        encoder, decoder = build_model(params, dico, gpu=False)\n",
    "    encoder, decoder = encoder[0], decoder[0]\n",
    "    d.update(time_forward(encoder, decoder, langs1, langs2, len1, len2, pred_mask, None, x1, x2, y))\n",
    "    torch.save((encoder.state_dict(), decoder.state_dict()), TMP_PATH)\n",
    "    d[\"size\"] = os.path.getsize(TMP_PATH) / 1024 / 1024\n",
    "    d[\"enc_ps\"] = sum([p.numel() for p in encoder.parameters() if p.requires_grad])\n",
    "    d[\"dec_ps\"] = sum([p.numel() for p in decoder.parameters() if p.requires_grad])\n",
    "    d[\"sum_ps\"] = d[\"enc_ps\"] + d[\"dec_ps\"]\n",
    "    d[\"emb_ps\"] = encoder.embeddings.weight.numel()\n",
    "    res[i] = d\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}